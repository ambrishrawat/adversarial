{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import skimage\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_img(img_filename):\n",
    "    \"\"\"\n",
    "    Load image from the filename. Default is to load in color if\n",
    "    possible.\n",
    "    Args:\n",
    "        img_name (string): string of the image name, relative to\n",
    "            the image directory.\n",
    "    Returns:\n",
    "        np array of float32: an image as a numpy array of float32\n",
    "    \"\"\"\n",
    "    img = skimage.img_as_float(imread(\n",
    "        img_filename,mode='RGB')).astype(np.float32)\n",
    "    if img.ndim == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "    elif img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "    \n",
    "    '''Subtract channel vise mean'''\n",
    "    ch_mean = np.load('preprocessing/ch_mean.npy')\n",
    "    img = img - ch_mean\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DictIterator():\n",
    "    '''\n",
    "    Class for iterating over a pandas data frame with filenames and addresses\n",
    "\n",
    "    TODO\n",
    "    add comments\n",
    "    '''\n",
    "    def __init__(self,csv_location = None,\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 target_size=(64,64)):\n",
    "        \n",
    "        if csv_location is not None:\n",
    "            self.df = pd.read_csv(csv_location)\n",
    "        self.N = self.df.shape[0]\n",
    "        print(self.N)\n",
    "        self.batch_size = batch_size\n",
    "        self.total_batches_seen = 0\n",
    "        self.index_gen = self._idx_gen(self.N,batch_size,shuffle)\n",
    "        self.target_size = target_size\n",
    "        self.img_set = None\n",
    "        \n",
    "    def _idx_gen(self,N,batch_size=32,shuffle=False):\n",
    "        batch_index = 0\n",
    "        while 1:\n",
    "            if batch_index == 0:\n",
    "                index_array = np.arange(N)\n",
    "                if shuffle:\n",
    "                    index_array = np.random.permutation(N)\n",
    "            current_index = (batch_index * batch_size) % N\n",
    "            if N >= current_index + batch_size:\n",
    "                current_batch_size = batch_size\n",
    "                batch_index += 1\n",
    "            else:\n",
    "                current_batch_size = N - current_index\n",
    "                batch_index = 0\n",
    "            self.total_batches_seen += 1\n",
    "            yield (index_array[current_index: current_index + current_batch_size],\n",
    "                   current_index, current_batch_size)\n",
    "            \n",
    "    def batch_gen(self):\n",
    "        while 1:\n",
    "            index_array, current_index, current_batch_size = self.index_gen.__next__()\n",
    "            img = [imresize(load_img(self.df.iloc[i]['filename']),size=self.target_size) for i in index_array]\n",
    "            img = np.array(img)\n",
    "            lab = [self.df.iloc[i].iloc[1:].values.astype('float32') for i in index_array]\n",
    "            lab = np.array(lab)\n",
    "            print(img.shape)\n",
    "            print(lab.shape)\n",
    "            plt.imshow(img[0])\n",
    "            yield img,lab\n",
    "    \n",
    "    def channel_mean(self):\n",
    "        self.img_set = np.array([load_img(self.df.iloc[i]['filename']) for i in range(self.N)])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "d = DictIterator(csv_location='preprocessing/trainset.csv')\n",
    "d.channel_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(d.img_set.shape)\n",
    "r = np.mean(d.img_set[:,:,:,0],  axis=0)\n",
    "g = np.mean(d.img_set[:,:,:,1],  axis=0)\n",
    "b = np.mean(d.img_set[:,:,:,2],  axis=0)\n",
    "\n",
    "\n",
    "\n",
    "np.save('preprocessing/r.npy',r)\n",
    "np.save('preprocessing/g.npy',g)\n",
    "np.save('preprocessing/b.npy',b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "ch_mean = np.zeros((64,64,3))\n",
    "print(ch_mean.shape)\n",
    "ch_mean[:,:,0] = r\n",
    "ch_mean[:,:,1] = g\n",
    "ch_mean[:,:,2] = b\n",
    "np.save('preprocessing/ch_mean.npy',ch_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model_defs import *\n",
    "from utils import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_modelx(csv_location='preprocessing/valset.csv',batch_size=64,mid='m1'):\n",
    "\n",
    "\n",
    "    #define the batch generator   (validation set)\n",
    "    val_datagen = CSVGenerator(csv_location=csv_location,\n",
    "                                 batch_size=batch_size)\n",
    "\n",
    "    val_generator = val_datagen.batch_gen()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 64, 64, 3))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('models/'+mid+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"models/\"+mid+\".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    d = val_generator.__next__()\n",
    "    pred = model(x)\n",
    "    pred\n",
    "    # evaluate loaded model on test data\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #val_loss = model.evaluate_generator(\n",
    "    #   generator = val_generator,\n",
    "    #   val_samples = val_datagen.get_data_size())\n",
    "\n",
    "    #print(\"%s: %.2f%%\" % (model.metrics_names[1], val_loss[1]*100))\n",
    "    #print(val_loss)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "try_modelx()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directory = '/dccstor/dlw/ambrish/examples/adversarial/models/m1'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mid = 'm1'\n",
    "epoch = 100\n",
    "val_loss = [1.32,23.4]\n",
    "with open('models/'+mid+'/acc_log.csv','a') as resultFile:\n",
    "    r = np.concatenate([np.asarray([epoch]),val_loss])\n",
    "    wr = csv.writer(resultFile,lineterminator='\\n')\n",
    "    \n",
    "    wr.writerows([r])\n",
    "    wr.writerow(['f','r','r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import skimage\n",
    "from skimage import io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "class tinyImageNet(object):\n",
    "\n",
    "    def __init__(self,fpath=None):\n",
    "        self.fpath = fpath\n",
    "        self.classes = None\n",
    "\n",
    "    def make_train_csv(self):\n",
    "        '''\n",
    "        for tiny ImageNet folder structure (train)\n",
    "        '''\n",
    "        self.classes = listdir(os.path.join(self.fpath,'train'))\n",
    "        num_classes = len(self.classes)\n",
    "        traincsv_file = Path('trainset.csv')\n",
    "        if traincsv_file.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            with open('trainset.csv', 'wt') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "                csvwriter.writerow(['filename']+self.classes) #csv header\n",
    "    \n",
    "                c_idx = 0\n",
    "                for class_ in self.classes:\n",
    "                    class_path = os.path.join(self.fpath,'train',class_,'images')\n",
    "                    images_ = listdir(class_path)\n",
    "                    for image_ in images_:\n",
    "    \n",
    "                        file_path = os.path.join(class_path,image_)\n",
    "    \n",
    "                        label = np.zeros(num_classes)\n",
    "                        label[c_idx] = 1\n",
    "    \n",
    "                        csvwriter.writerow([file_path] + list(map(str, label)))\n",
    "                    c_idx+=1\n",
    "\n",
    "    def make_val_csv(self):\n",
    "        '''\n",
    "        for tiny ImageNet folder structure (val)\n",
    "        '''\n",
    "        valcsv_file = Path('val.csv')\n",
    "        if valcsv_file.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(self.fpath,'val/val_annotations.txt'), 'rt') as csvfile:\n",
    "                \n",
    "                reader = csv.reader(csvfile, delimiter='\\t')\n",
    "                \n",
    "                with open('valset.csv', 'wt') as csvfile:\n",
    "                    \n",
    "                    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "                    csvwriter.writerow(['filename']+self.classes)\n",
    "                    \n",
    "                    for row in reader:\n",
    "\n",
    "                        file_path = os.path.join(self.fpath,'val','images',row[0])\n",
    "                        \n",
    "                        label = np.zeros(len(self.classes))\n",
    "                        label[self.classes.index(row[1])] = 1\n",
    "                        \n",
    "                        csvwriter.writerow([file_path] + list(map(str, label)))            \n",
    "                \n",
    "    def make_csvs(self):\n",
    "        self.make_train_csv()\n",
    "        self.make_val_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "from scipy.misc import imsave\n",
    "# Width and height of each image.\n",
    "img_size = 32\n",
    "\n",
    "# Number of channels in each image, 3 channels: Red, Green, Blue.\n",
    "num_channels = 3\n",
    "\n",
    "# Length of an image when flattened to a 1-dim array.\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 10\n",
    "\n",
    "# Number of files for the training-set.\n",
    "_num_files_train = 5\n",
    "\n",
    "# Number of images for each batch-file in the training-set.\n",
    "_images_per_file = 10000\n",
    "\n",
    "# Total number of images in the training-set.\n",
    "# This is used to pre-allocate arrays for efficiency.\n",
    "_num_images_train = _num_files_train * _images_per_file\n",
    "\n",
    "classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "\n",
    "def save_figs(dest_path=None, mode=None, cls=None, filenames=None, images=None):\n",
    "    '''helper function to save images'''\n",
    "    if mode=='train':\n",
    "        for i in range(filenames.shape[0]):\n",
    "            imsave(os.path.join(dest_path,mode,classes[cls[i]],filenames[i].decode('ascii')),images[i])\n",
    "    else:\n",
    "        for i in range(filenames.shape[0]):\n",
    "            imsave(os.path.join(dest_path,mode,'images',filenames[i].decode('ascii')),images[i])\n",
    "            with open(os.path.join(dest_path,mode,'annotations.csv'), 'a') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "                csvwriter.writerow([filenames[i].decode('ascii'),classes[cls[i]]]) #csv header\n",
    "            # save this in annotations classes[cls[i]]\n",
    "        \n",
    "        \n",
    "    pass\n",
    "\n",
    "def _convert_images(raw):\n",
    "    \"\"\"\n",
    "    Convert images from the CIFAR-10 format and\n",
    "    return a 4-dim array with shape: [image_number, height, width, channel]\n",
    "    where the pixels are floats between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the raw images from the data-files to floating-points.\n",
    "    raw_float = np.array(raw, dtype=float) / 255.0\n",
    "\n",
    "    # Reshape the array to 4-dimensions.\n",
    "    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n",
    "\n",
    "    # Reorder the indices of the array.\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images\n",
    "\n",
    "def _unpickle(file):\n",
    "    '''Unpickle the files (use bytes encoding for compatibility with Python3)'''\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo,encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "class cifar10(object):\n",
    "    '''\n",
    "    Preprocessing the data set and making AdvFlow compatible csv files for CIFAR10\n",
    "    Also, saves the JPEGs for the trianing and test set which facilitates\n",
    "    easy visualisation of adversarial images\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self,fpath=None):\n",
    "        self.fpath = fpath\n",
    "        if fpath is None:\n",
    "            raise ValueError\n",
    "        self.classes = 10\n",
    "    \n",
    "    def save_jpegs(self,source_path=None):\n",
    "        \n",
    "        ''' saves JPEGs structured into class wise sub folders'''\n",
    "        \n",
    "        dest_path = self.fpath\n",
    "        if source_path is None:\n",
    "            raise ValueError\n",
    "            \n",
    "        files = os.listdir(source_path)\n",
    "        batches = ['data_batch_1','data_batch_2', 'data_batch_3','data_batch_4', 'data_batch_5', 'test_batch']\n",
    "        \n",
    "        #train_batches = batches[0:5]\n",
    "        #test_batches = batches[5:6]\n",
    "        \n",
    "        from functools import reduce\n",
    "        test = reduce(lambda x,y : x and y, [b in files for b in batches])\n",
    "        if test is False:\n",
    "            raise ValueError\n",
    "            \n",
    "        #make class dirs\n",
    "        if not(os.path.exists(dest_path)):\n",
    "            os.makedirs(dest_path)\n",
    "        \n",
    "        for c in classes:\n",
    "            if not(os.path.exists(os.path.join(dest_path,'train',c))):\n",
    "                os.makedirs(os.path.join(dest_path,'train',c))\n",
    "        if not(os.path.exists(os.path.join(dest_path,'test','images'))):\n",
    "            os.makedirs(os.path.join(dest_path,'test','images'))\n",
    "        \n",
    "\n",
    "        for b in batches:\n",
    "            \n",
    "            # Load the pickled data-file.\n",
    "            data = _unpickle(source_path+'/'+b)\n",
    "            \n",
    "            print(data.keys())\n",
    "\n",
    "            # Get the raw images.\n",
    "            raw_images = data[b'data']\n",
    "        \n",
    "            # Get the class-numbers for each image. Convert to numpy-array.\n",
    "            cls = np.array(data[b'labels'])\n",
    "\n",
    "            #filenames\n",
    "            filenames = np.array(data[b'filenames'])\n",
    "\n",
    "            # Convert the images.\n",
    "            images = _convert_images(raw_images)\n",
    "            print(images.shape)\n",
    "            \n",
    "            # save images as jpegs\n",
    "            if b == 'test_batch':\n",
    "                save_figs(dest_path=dest_path, mode='test',cls=cls, filenames=filenames, images=images)\n",
    "            else:\n",
    "                save_figs(dest_path=dest_path, mode='train',cls=cls, filenames=filenames, images=images)\n",
    "            \n",
    "\n",
    "    def make_train_csv(self, traincsv=None):\n",
    "        '''\n",
    "        for cifar10_sets folder structure (train)\n",
    "        '''\n",
    "        self.classes = classes\n",
    "        num_classes = len(self.classes)\n",
    "        \n",
    "        traincsv_file = Path(traincsv)\n",
    "        if False:\n",
    "            pass\n",
    "        else:\n",
    "            with open(traincsv, 'wt') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "                csvwriter.writerow(['filename']+self.classes) #csv header\n",
    "    \n",
    "                c_idx = 0\n",
    "                for class_ in self.classes:\n",
    "                    class_path = os.path.join(self.fpath,'train',class_)\n",
    "                    images_ = listdir(class_path)\n",
    "                    for image_ in images_:\n",
    "    \n",
    "                        file_path = os.path.join(class_path,image_)\n",
    "    \n",
    "                        label = np.zeros(num_classes)\n",
    "                        label[c_idx] = 1\n",
    "    \n",
    "                        csvwriter.writerow([file_path] + list(map(str, label)))\n",
    "                    c_idx+=1\n",
    "\n",
    "    def make_test_csv(self, valcsv=None):\n",
    "        '''\n",
    "        for tiny ImageNet folder structure (val)\n",
    "        '''\n",
    "        valcsv_file = Path(valcsv)\n",
    "        if valcsv_file.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(self.fpath,'test','annotations.csv'), 'rt') as csvfile:\n",
    "                \n",
    "                reader = csv.reader(csvfile, delimiter='\\t')\n",
    "                \n",
    "                with open(valcsv, 'wt') as csvfile:\n",
    "                    \n",
    "                    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "                    csvwriter.writerow(['filename']+self.classes)\n",
    "                    \n",
    "                    for row in reader:\n",
    "\n",
    "                        file_path = os.path.join(self.fpath,'test','images',row[0])\n",
    "                        \n",
    "                        label = np.zeros(len(self.classes))\n",
    "                        label[self.classes.index(row[1])] = 1\n",
    "                        \n",
    "                        csvwriter.writerow([file_path] + list(map(str, label)))            \n",
    "                \n",
    "    def make_csvs(self):\n",
    "        self.make_train_csv(traincsv='/dccstor/dlw/ambrish/examples/adversarial/preprocessing/train_cifar10.csv')\n",
    "        self.make_test_csv(valcsv='/dccstor/dlw/ambrish/examples/adversarial/preprocessing/test_cifar10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = cifar10(fpath='/dccstor/dlw/ambrish/data/cifar10/cifar10-set/')\n",
    "#c.save_jpegs(source_path='/dccstor/dlw/ambrish/data/cifar10/cifar-10-batches-py/')\n",
    "c.make_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
